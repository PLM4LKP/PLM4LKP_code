{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自己预训练的模型\n",
    "MAX_TEXT_LEN = 512\n",
    "MAX_CODE_LEN = 256\n",
    "\n",
    "from torch import nn, tensor, cat, load\n",
    "\n",
    "class RelatednessModel(nn.Module):\n",
    "    def __init__(self, clas, tags_count, freeze_embedder=False, pretrained_model_path=None, dropout_prob=0.1, hidden_size=4096):\n",
    "        super(RelatednessModel, self).__init__()\n",
    "        print('加载模型')\n",
    "        self.embedder = clas(tags_count)\n",
    "        if pretrained_model_path != None:\n",
    "            self.embedder.load_state_dict(load(pretrained_model_path))\n",
    "        if freeze_embedder:\n",
    "            self.embedder.freeze_bert()\n",
    "\n",
    "        self.dropout_embeddings = nn.Dropout(dropout_prob)\n",
    "        self.dense_hidden = nn.Linear(768 * clas.embedders * 2, hidden_size)\n",
    "        self.relu_hidden = nn.ReLU()\n",
    "        self.dropout_hidden = nn.Dropout(dropout_prob)\n",
    "        self.dense_classifaction = nn.Linear(hidden_size, 4)\n",
    "        # self.dense.weight.data.normal_(0, 0.01)\n",
    "        self.sigmoid_classifaction = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, batch_title1, batch_code1, batch_desc1, batch_title2, batch_code2, batch_desc2):\n",
    "        embeddings1 = self.embedder(batch_title1, batch_code1, batch_desc1)[1]\n",
    "        embeddings2 = self.embedder(batch_title2, batch_code2, batch_desc2)[1]\n",
    "\n",
    "        embeddings = cat((embeddings1, embeddings2), dim=1)\n",
    "        dropout_embeddings_result = self.dropout_hidden(embeddings)\n",
    "        dense_hidden_result = self.dense_hidden(dropout_embeddings_result)\n",
    "        relu_hidden_result = self.relu_hidden(dense_hidden_result)\n",
    "        dropout_hidden_result = self.dropout_embeddings(relu_hidden_result)\n",
    "        dense_classifaction = self.dense_classifaction(dropout_hidden_result)\n",
    "        classifaction = self.sigmoid_classifaction(dense_classifaction)\n",
    "        return classifaction, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自己预训练的模型，两路合并\n",
    "MAX_TEXT_LEN = 512\n",
    "MAX_CODE_LEN = 256\n",
    "\n",
    "from torch import nn, tensor, cat, load\n",
    "\n",
    "class RelatednessModelCross(nn.Module):\n",
    "    def __init__(self, clas, tags_count, freeze_embedder=False, pretrained_model_path=None, dropout_prob=0.1, hidden_size=4096, code_model=False,\n",
    "            text_cls_token_id=101, text_sep_token_id=102, code_cls_token_id=0, code_sep_token_id=2):\n",
    "        super(RelatednessModelCross, self).__init__()\n",
    "        print('加载模型')\n",
    "        self.embedder = clas(tags_count)\n",
    "        if pretrained_model_path != None:\n",
    "            self.embedder.load_state_dict(load(pretrained_model_path))\n",
    "        if freeze_embedder:\n",
    "            self.embedder.freeze_bert()\n",
    "\n",
    "        self.dropout_embeddings = nn.Dropout(dropout_prob)\n",
    "        self.dense_hidden = nn.Linear(768 * clas.embedders, hidden_size)\n",
    "        self.relu_hidden = nn.ReLU()\n",
    "        self.dropout_hidden = nn.Dropout(dropout_prob)\n",
    "        self.dense_classifaction = nn.Linear(hidden_size, 4)\n",
    "        # self.dense.weight.data.normal_(0, 0.01)\n",
    "        self.sigmoid_classifaction = nn.Sigmoid()\n",
    "\n",
    "        self.code_model = code_model\n",
    "        self.text_cls_token_id = text_cls_token_id\n",
    "        self.text_sep_token_id = text_sep_token_id\n",
    "        self.code_cls_token_id = code_cls_token_id\n",
    "        self.code_sep_token_id = code_sep_token_id\n",
    "    \n",
    "    def tokenize(self, tokenizer, batch_sentences, length):\n",
    "        encoded = tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=False, truncation=True, max_length=length)\n",
    "        return encoded\n",
    "    \n",
    "    def embedding(self, bert, batch_input_ids, batch_attention_mask):\n",
    "        tokens_ids = tensor(batch_input_ids).cuda()\n",
    "        attention_mask = tensor(batch_attention_mask).cuda()\n",
    "        bert_result = bert(input_ids=tokens_ids, attention_mask=attention_mask)\n",
    "        # embeddings = bert_result[0][:,0,:].contiguous()\n",
    "        embeddings = bert_result[1]\n",
    "        return embeddings\n",
    "\n",
    "    def cross_embedding(self, bert, tokenizer, batch1, batch2, cls_token_id, sep_token_id, max_len):\n",
    "        tokenized1 = self.tokenize(tokenizer, batch1, max_len)\n",
    "        tokenized2 = self.tokenize(tokenizer, batch2, max_len)\n",
    "        \n",
    "        input_ids1 = tokenized1['input_ids']\n",
    "        input_ids2 = tokenized2['input_ids']\n",
    "\n",
    "        batch_input_ids = []\n",
    "        batch_attention_masks = []\n",
    "        for i in range(len(input_ids1)):\n",
    "            input_idsi1 = input_ids1[i]\n",
    "            input_idsi2 = input_ids2[i]\n",
    "            if len(input_idsi1) + len(input_idsi2) > max_len - 3:\n",
    "                s = len(input_idsi1) + len(input_idsi2)\n",
    "                input_idsi1 = input_idsi1[:len(input_idsi1) * (max_len - 3) // s]\n",
    "                input_idsi2 = input_idsi2[:len(input_idsi2) * (max_len - 3) // s]\n",
    "            input_ids = [cls_token_id] + input_idsi1 + [sep_token_id]+ input_idsi2 + [sep_token_id]\n",
    "            attention_mask = [1 for i in range(len(input_ids))]\n",
    "            attention_mask += [0 for i in range(max_len - len(input_ids))]\n",
    "            input_ids += [0 for i in range(max_len - len(input_ids))]\n",
    "            attention_mask += [0 for i in range(max_len - len(input_ids))]\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attention_masks.append(attention_mask)\n",
    "            \n",
    "        embeddings = self.embedding(bert, batch_input_ids, batch_attention_masks)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, batch_title1, batch_code1, batch_desc1, batch_title2, batch_code2, batch_desc2):\n",
    "        batch_text1 = []\n",
    "        batch_text2 = []\n",
    "        for i in range(len(batch_title1)):\n",
    "            text1 = batch_title1[i] + ' ' + batch_desc1[i]\n",
    "            text2 = batch_title2[i] + ' ' + batch_desc2[i]\n",
    "            batch_text1.append(text1)\n",
    "            batch_text2.append(text2)\n",
    "\n",
    "        if self.code_model:\n",
    "            embeddings_text = self.cross_embedding(self.embedder.bert_text, self.embedder.tokenizer_text, batch_text1, batch_text2, self.text_cls_token_id, self.text_sep_token_id, MAX_TEXT_LEN)\n",
    "            embeddings_code = self.cross_embedding(self.embedder.bert_code, self.embedder.tokenizer_code, batch_code1, batch_code2, self.code_cls_token_id, self.code_sep_token_id, MAX_CODE_LEN)\n",
    "            embeddings = cat((embeddings_text, embeddings_code), 1)\n",
    "        else:\n",
    "            embeddings = self.cross_embedding(self.embedder.bert_text, self.embedder.tokenizer_text, batch_text1, batch_text2, self.text_cls_token_id, self.text_sep_token_id, MAX_TEXT_LEN)\n",
    "        \n",
    "        dropout_embeddings_result = self.dropout_hidden(embeddings)\n",
    "        dense_hidden_result = self.dense_hidden(dropout_embeddings_result)\n",
    "        relu_hidden_result = self.relu_hidden(dense_hidden_result)\n",
    "        dropout_hidden_result = self.dropout_embeddings(relu_hidden_result)\n",
    "        dense_classifaction = self.dense_classifaction(dropout_hidden_result)\n",
    "        classifaction = self.sigmoid_classifaction(dense_classifaction)\n",
    "        return classifaction, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BtdCModel import TagRecommandModel\n",
    "model = RelatednessModelCross(TagRecommandModel, 23687, pretrained_model_path='大数据量/2022-04-08 041514-epoch1.dat', freeze_embedder=True, hidden_size=4096)\n",
    "\n",
    "batch_sentences = ['test aaa hahaha', 'test aaa hahaha']\n",
    "model = model.cuda()\n",
    "print(model(batch_sentences, batch_sentences, batch_sentences, batch_sentences, batch_sentences, batch_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用预训练的公共模型\n",
    "MAX_TEXT_LEN = 512\n",
    "MAX_CODE_LEN = 256\n",
    "\n",
    "from torch import nn, tensor, cat, load\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class RelatednessModel(nn.Module):\n",
    "    def __init__(self, ptm_name, freeze_embedder=False,dropout_prob=0.1, hidden_size=4096):\n",
    "        super(RelatednessModel, self).__init__()\n",
    "        print('加载模型')\n",
    "        self.tokenizer_text = AutoTokenizer.from_pretrained(ptm_name)\n",
    "\n",
    "        self.bert_text = AutoModel.from_pretrained(ptm_name)\n",
    "\n",
    "        if freeze_embedder:\n",
    "            for param in self.bert_text.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout_embeddings = nn.Dropout(dropout_prob)\n",
    "        self.dense_hidden = nn.Linear(768 * 1 * 2, hidden_size)\n",
    "        self.dense_hidden.weight.data.normal_(0, 0.01)\n",
    "        self.relu_hidden = nn.ReLU()\n",
    "        self.dropout_hidden = nn.Dropout(dropout_prob)\n",
    "        self.dense_classifaction = nn.Linear(hidden_size, 4)\n",
    "        self.dense_classifaction.weight.data.normal_(0, 0.01)\n",
    "        self.sigmoid_classifaction = nn.Sigmoid()\n",
    "\n",
    "    def part_embedding(self, tokenizer, bert, batch_sentences, length):\n",
    "        encoded = tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=True, truncation=True, max_length=length, padding='max_length')\n",
    "        tokens_ids = tensor(encoded['input_ids']).cuda()\n",
    "        attention_mask = tensor(encoded['attention_mask']).cuda()\n",
    "        bert_result = bert(input_ids=tokens_ids, attention_mask=attention_mask)\n",
    "        # embeddings = bert_result[0][:,0,:].contiguous()\n",
    "        embeddings = bert_result[1]\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, batch_title1, batch_code1, batch_desc1, batch_title2, batch_code2, batch_desc2):\n",
    "        batch_text1 = [batch_title1[i] + ' ' + batch_desc1[i] for i in range(len(batch_title1))]\n",
    "        embeddings_text1 = self.part_embedding(self.tokenizer_text, self.bert_text, batch_text1, MAX_TEXT_LEN)\n",
    "        batch_text2 = [batch_title2[i] + ' ' + batch_desc2[i] for i in range(len(batch_title2))]\n",
    "        embeddings_text2 = self.part_embedding(self.tokenizer_text, self.bert_text, batch_text2, MAX_TEXT_LEN)\n",
    "\n",
    "        embeddings = cat((embeddings_text1, embeddings_text2), dim=1)\n",
    "        dropout_embeddings_result = self.dropout_hidden(embeddings)\n",
    "        dense_hidden_result = self.dense_hidden(dropout_embeddings_result)\n",
    "        relu_hidden_result = self.relu_hidden(dense_hidden_result)\n",
    "        dropout_hidden_result = self.dropout_embeddings(relu_hidden_result)\n",
    "        dense_classifaction = self.dense_classifaction(dropout_hidden_result)\n",
    "        classifaction = self.sigmoid_classifaction(dense_classifaction)\n",
    "        return classifaction, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用预训练的公共模型，两句直接合并\n",
    "MAX_TEXT_LEN = 512\n",
    "MAX_CODE_LEN = 256\n",
    "\n",
    "from torch import nn, tensor, cat, load\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class RelatednessModelCross(nn.Module):\n",
    "    def __init__(self, ptm_name, freeze_embedder=False,dropout_prob=0.1, hidden_size=4096, cls_token_id=101, sep_token_id=102):\n",
    "        super(RelatednessModelCross, self).__init__()\n",
    "        print('加载模型')\n",
    "        self.tokenizer_text = AutoTokenizer.from_pretrained(ptm_name)\n",
    "\n",
    "        self.bert_text = AutoModel.from_pretrained(ptm_name)\n",
    "\n",
    "        if freeze_embedder:\n",
    "            for param in self.bert_text.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout_embeddings = nn.Dropout(dropout_prob)\n",
    "        self.dense_hidden = nn.Linear(768 * 1 * 1, hidden_size)\n",
    "        self.dense_hidden.weight.data.normal_(0, 0.01)\n",
    "        self.relu_hidden = nn.ReLU()\n",
    "        self.dropout_hidden = nn.Dropout(dropout_prob)\n",
    "        self.dense_classifaction = nn.Linear(hidden_size, 4)\n",
    "        self.dense_classifaction.weight.data.normal_(0, 0.01)\n",
    "        self.sigmoid_classifaction = nn.Sigmoid()\n",
    "        self.cls = cls_token_id\n",
    "        self.sep = sep_token_id\n",
    "\n",
    "    def tokenize(self, tokenizer, batch_sentences, length):\n",
    "        encoded = tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=False, truncation=True, max_length=length)\n",
    "        return encoded\n",
    "    \n",
    "    def embedding(self, bert, batch_input_ids, batch_attention_mask):\n",
    "        tokens_ids = tensor(batch_input_ids).cuda()\n",
    "        attention_mask = tensor(batch_attention_mask).cuda()\n",
    "        bert_result = bert(input_ids=tokens_ids, attention_mask=attention_mask)\n",
    "        # embeddings = bert_result[0][:,0,:].contiguous()\n",
    "        embeddings = bert_result[1]\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, batch_title1, batch_code1, batch_desc1, batch_title2, batch_code2, batch_desc2):\n",
    "        batch_text1 = []\n",
    "        batch_text2 = []\n",
    "        for i in range(len(batch_title1)):\n",
    "            text1 = batch_title1[i] + ' ' + batch_desc1[i]\n",
    "            text2 = batch_title2[i] + ' ' + batch_desc2[i]\n",
    "            batch_text1.append(text1)\n",
    "            batch_text2.append(text2)\n",
    "        tokenized1 = self.tokenize(self.tokenizer_text, batch_text1, MAX_TEXT_LEN)\n",
    "        tokenized2 = self.tokenize(self.tokenizer_text, batch_text2, MAX_TEXT_LEN)\n",
    "        \n",
    "        input_ids1 = tokenized1['input_ids']\n",
    "        input_ids2 = tokenized2['input_ids']\n",
    "\n",
    "        batch_input_ids = []\n",
    "        batch_attention_masks = []\n",
    "        for i in range(len(input_ids1)):\n",
    "            input_idsi1 = input_ids1[i]\n",
    "            input_idsi2 = input_ids2[i]\n",
    "            if len(input_idsi1) + len(input_idsi2) > MAX_TEXT_LEN - 3:\n",
    "                s = len(input_idsi1) + len(input_idsi2)\n",
    "                input_idsi1 = input_idsi1[:len(input_idsi1) * (MAX_TEXT_LEN - 3) // s]\n",
    "                input_idsi2 = input_idsi2[:len(input_idsi2) * (MAX_TEXT_LEN - 3) // s]\n",
    "            input_ids = [self.cls] + input_idsi1 + [self.sep]+ input_idsi2 + [self.sep]\n",
    "            attention_mask = [1 for i in range(len(input_ids))]\n",
    "            attention_mask += [0 for i in range(MAX_TEXT_LEN - len(input_ids))]\n",
    "            input_ids += [0 for i in range(MAX_TEXT_LEN - len(input_ids))]\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attention_masks.append(attention_mask)\n",
    "            \n",
    "        embeddings = self.embedding(self.bert_text, batch_input_ids, batch_attention_masks)\n",
    "\n",
    "        dropout_embeddings_result = self.dropout_hidden(embeddings)\n",
    "        dense_hidden_result = self.dense_hidden(dropout_embeddings_result)\n",
    "        relu_hidden_result = self.relu_hidden(dense_hidden_result)\n",
    "        dropout_hidden_result = self.dropout_embeddings(relu_hidden_result)\n",
    "        dense_classifaction = self.dense_classifaction(dropout_hidden_result)\n",
    "        classifaction = self.sigmoid_classifaction(dense_classifaction)\n",
    "        return classifaction, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用预训练的公共模型，两句直接合并，减小后置层数\n",
    "MAX_TEXT_LEN = 512\n",
    "MAX_CODE_LEN = 256\n",
    "\n",
    "from torch import nn, tensor, cat, load\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class RelatednessModelCross(nn.Module):\n",
    "    def __init__(self, ptm_name, freeze_embedder=False,dropout_prob=0.1, hidden_size=4096, cls_token_id=101, sep_token_id=102):\n",
    "        super(RelatednessModelCross, self).__init__()\n",
    "        print('加载模型')\n",
    "        self.tokenizer_text = AutoTokenizer.from_pretrained(ptm_name)\n",
    "\n",
    "        self.bert_text = AutoModel.from_pretrained(ptm_name)\n",
    "\n",
    "        if freeze_embedder:\n",
    "            for param in self.bert_text.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout_embeddings = nn.Dropout(dropout_prob)\n",
    "        self.dense_classifaction = nn.Linear(768 * 1 * 1, 4)\n",
    "        self.dense_classifaction.weight.data.normal_(0, 0.01)\n",
    "        self.sigmoid_classifaction = nn.Sigmoid()\n",
    "        self.cls = cls_token_id\n",
    "        self.sep = sep_token_id\n",
    "\n",
    "    def tokenize(self, tokenizer, batch_sentences, length):\n",
    "        encoded = tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=False, truncation=True, max_length=length)\n",
    "        return encoded\n",
    "    \n",
    "    def embedding(self, bert, batch_input_ids, batch_attention_mask):\n",
    "        tokens_ids = tensor(batch_input_ids).cuda()\n",
    "        attention_mask = tensor(batch_attention_mask).cuda()\n",
    "        bert_result = bert(input_ids=tokens_ids, attention_mask=attention_mask)\n",
    "        # embeddings = bert_result[0][:,0,:].contiguous()\n",
    "        embeddings = bert_result[1]\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, batch_title1, batch_code1, batch_desc1, batch_title2, batch_code2, batch_desc2):\n",
    "        batch_text1 = []\n",
    "        batch_text2 = []\n",
    "        for i in range(len(batch_title1)):\n",
    "            text1 = batch_title1[i] + ' ' + batch_desc1[i]\n",
    "            text2 = batch_title2[i] + ' ' + batch_desc2[i]\n",
    "            batch_text1.append(text1)\n",
    "            batch_text2.append(text2)\n",
    "        tokenized1 = self.tokenize(self.tokenizer_text, batch_text1, MAX_TEXT_LEN)\n",
    "        tokenized2 = self.tokenize(self.tokenizer_text, batch_text2, MAX_TEXT_LEN)\n",
    "        \n",
    "        input_ids1 = tokenized1['input_ids']\n",
    "        input_ids2 = tokenized2['input_ids']\n",
    "\n",
    "        batch_input_ids = []\n",
    "        batch_attention_masks = []\n",
    "        for i in range(len(input_ids1)):\n",
    "            input_idsi1 = input_ids1[i]\n",
    "            input_idsi2 = input_ids2[i]\n",
    "            if len(input_idsi1) + len(input_idsi2) > MAX_TEXT_LEN - 3:\n",
    "                s = len(input_idsi1) + len(input_idsi2)\n",
    "                input_idsi1 = input_idsi1[:len(input_idsi1) * (MAX_TEXT_LEN - 3) // s]\n",
    "                input_idsi2 = input_idsi2[:len(input_idsi2) * (MAX_TEXT_LEN - 3) // s]\n",
    "            input_ids = [self.cls] + input_idsi1 + [self.sep]+ input_idsi2 + [self.sep]\n",
    "            attention_mask = [1 for i in range(len(input_ids))]\n",
    "            attention_mask += [0 for i in range(MAX_TEXT_LEN - len(input_ids))]\n",
    "            input_ids += [0 for i in range(MAX_TEXT_LEN - len(input_ids))]\n",
    "            attention_mask += [0 for i in range(MAX_TEXT_LEN - len(input_ids))]\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attention_masks.append(attention_mask)\n",
    "            \n",
    "        embeddings = self.embedding(self.bert_text, batch_input_ids, batch_attention_masks)\n",
    "\n",
    "        dropout_hidden_result = self.dropout_embeddings(embeddings)\n",
    "        dense_classifaction = self.dense_classifaction(dropout_hidden_result)\n",
    "        classifaction = self.sigmoid_classifaction(dense_classifaction)\n",
    "        return classifaction, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-encoding + TAPT + hidden-layer\n",
    "MAX_TEXT_LEN = 512\n",
    "MAX_CODE_LEN = 256\n",
    "\n",
    "from torch import nn, Tensor, tensor, cat, load\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class RelatednessModelCrossHidden(nn.Module):\n",
    "    def __init__(self, ptm_name, freeze_embedder=False,dropout_prob=0.1, hidden_size=4096, cls_token_id=101, sep_token_id=102, used_hidden_layer=1):\n",
    "        super(RelatednessModelCrossHidden, self).__init__()\n",
    "        print('加载模型')\n",
    "        self.tokenizer_text = AutoTokenizer.from_pretrained(ptm_name)\n",
    "\n",
    "        self.bert_text = AutoModel.from_pretrained(ptm_name)\n",
    "\n",
    "        if freeze_embedder:\n",
    "            for param in self.bert_text.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout_embeddings = nn.Dropout(dropout_prob)\n",
    "        self.dense_classifaction = nn.Linear(768 * 1 * 1, 4)\n",
    "        self.dense_classifaction.weight.data.normal_(0, 0.01)\n",
    "        self.sigmoid_classifaction = nn.Sigmoid()\n",
    "        self.cls = cls_token_id\n",
    "        self.sep = sep_token_id\n",
    "        self.used_hidden_layer = used_hidden_layer\n",
    "        self.pooler = BertPooler(self.bert_text.config)\n",
    "\n",
    "    def tokenize(self, tokenizer, batch_sentences, length):\n",
    "        encoded = tokenizer.batch_encode_plus(batch_sentences, add_special_tokens=False, truncation=True, max_length=length)\n",
    "        return encoded\n",
    "    \n",
    "    def embedding(self, bert, batch_input_ids, batch_attention_mask):\n",
    "        tokens_ids = tensor(batch_input_ids).cuda()\n",
    "        attention_mask = tensor(batch_attention_mask).cuda()\n",
    "        bert_result = bert(input_ids=tokens_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        embeddings = bert.pooler(bert_result.hidden_states[self.used_hidden_layer])\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, batch_title1, batch_code1, batch_desc1, batch_title2, batch_code2, batch_desc2):\n",
    "        batch_text1 = []\n",
    "        batch_text2 = []\n",
    "        for i in range(len(batch_title1)):\n",
    "            text1 = batch_title1[i] + ' ' + batch_desc1[i]\n",
    "            text2 = batch_title2[i] + ' ' + batch_desc2[i]\n",
    "            batch_text1.append(text1)\n",
    "            batch_text2.append(text2)\n",
    "        tokenized1 = self.tokenize(self.tokenizer_text, batch_text1, MAX_TEXT_LEN)\n",
    "        tokenized2 = self.tokenize(self.tokenizer_text, batch_text2, MAX_TEXT_LEN)\n",
    "        \n",
    "        input_ids1 = tokenized1['input_ids']\n",
    "        input_ids2 = tokenized2['input_ids']\n",
    "\n",
    "        batch_input_ids = []\n",
    "        batch_attention_masks = []\n",
    "        for i in range(len(input_ids1)):\n",
    "            input_idsi1 = input_ids1[i]\n",
    "            input_idsi2 = input_ids2[i]\n",
    "            if len(input_idsi1) + len(input_idsi2) > MAX_TEXT_LEN - 3:\n",
    "                s = len(input_idsi1) + len(input_idsi2)\n",
    "                input_idsi1 = input_idsi1[:len(input_idsi1) * (MAX_TEXT_LEN - 3) // s]\n",
    "                input_idsi2 = input_idsi2[:len(input_idsi2) * (MAX_TEXT_LEN - 3) // s]\n",
    "            input_ids = [self.cls] + input_idsi1 + [self.sep]+ input_idsi2 + [self.sep]\n",
    "            attention_mask = [1 for i in range(len(input_ids))]\n",
    "            attention_mask += [0 for i in range(MAX_TEXT_LEN - len(input_ids))]\n",
    "            input_ids += [0 for i in range(MAX_TEXT_LEN - len(input_ids))]\n",
    "            attention_mask += [0 for i in range(MAX_TEXT_LEN - len(input_ids))]\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attention_masks.append(attention_mask)\n",
    "            \n",
    "        embeddings = self.embedding(self.bert_text, batch_input_ids, batch_attention_masks)\n",
    "\n",
    "        dropout_hidden_result = self.dropout_embeddings(embeddings)\n",
    "        dense_classifaction = self.dense_classifaction(dropout_hidden_result)\n",
    "        classifaction = self.sigmoid_classifaction(dense_classifaction)\n",
    "        return classifaction, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'roberta-base'\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModel.from_pretrained(name)\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "text = \"<s>Replace me by any text you'd like.</s>\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "# output = model(**encoded_input)\n",
    "# print(encoded_input)\n",
    "# print(output[0])\n",
    "# print(output[1])\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BtdModel import TagRecommandModel\n",
    "batch_sentences = ['test aaa hahaha', 'var i = 0', \"Replace me by any text you'd like.\"]\n",
    "model = RelatednessModelCrossHidden('bert-base-uncased', freeze_embedder=True)\n",
    "model = model.cuda()\n",
    "print(model(batch_sentences, batch_sentences, batch_sentences, batch_sentences, batch_sentences, batch_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch import tensor\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "TAGS_LIST_SAVE = '../data/mid/commonTags.json'\n",
    "\n",
    "class Dataset():\n",
    "    label_dict = {\n",
    "        'duplicate': 0,\n",
    "        'direct': 1,\n",
    "        'indirect': 2,\n",
    "        'isolated': 3\n",
    "    }\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.source = pd.read_csv(data_path, lineterminator=\"\\n\")\n",
    "        self.size = len(self.source)\n",
    "        self.data = []\n",
    "        for i in range(self.size):\n",
    "            item = self.source.iloc[i]\n",
    "            result = [0 for i in range(4)]\n",
    "            result[Dataset.label_dict[item['class'].strip()]] = 1\n",
    "            self.data.append({\n",
    "                'title1': item['q1_Title'].strip() if type(item['q1_Title']) == str else '',\n",
    "                'desc1': item['q1_Body'].strip() if type(item['q1_Body']) == str else '',\n",
    "                'code1': item['q1_BodyCode'].strip() if type(item['q1_BodyCode']) == str else '',\n",
    "                'title2': item['q2_Title'].strip() if type(item['q2_Title']) == str else '',\n",
    "                'desc2': item['q2_Body'].strip() if type(item['q2_Body']) == str else '',\n",
    "                'code2': item['q2_BodyCode'].strip() if type(item['q2_BodyCode']) == str else '',\n",
    "                'class': tensor(result).float()\n",
    "            })\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i >= self.size:\n",
    "            raise StopIteration\n",
    "        return self.data[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN = '../data/raw/medium_link_prediction_noClue_shuffled_train.csv'\n",
    "DATASET_TEST = '../data/raw/medium_link_prediction_noClue_shuffled_test.csv'\n",
    "\n",
    "a = pd.read_csv(DATASET_TEST)\n",
    "print(a.iloc[0]['q1_Id'])\n",
    "dataset = Dataset(DATASET_TEST)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_BATCH_SIZE = 8\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import topk, arange\n",
    "import torch\n",
    "try:\n",
    "    get_ipython().__class_._name__\n",
    "    from tqdm.notebook import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "LABEL_BACK_DICT = ['duplicate', 'direct', 'indirect', 'isolated']\n",
    "\n",
    "def val(model, val_dataset, show=False):\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "\n",
    "    c = [[0 for j in range(4)] for i in range(4)]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(val_dataloader)):\n",
    "            input_title1 = data['title1']\n",
    "            input_code1 = data['code1']\n",
    "            input_desc1 = data['desc1']\n",
    "            input_title2 = data['title2']\n",
    "            input_code2 = data['code2']\n",
    "            input_desc2 = data['desc2']\n",
    "            probability = model(input_title1, input_code1, input_desc1, input_title2, input_code2, input_desc2)[0]\n",
    "            tops = topk(probability, 1, sorted=True)\n",
    "            for indices, j in zip(tops.indices, arange(len(tops.indices))):\n",
    "                for k in range(4):\n",
    "                    if data['class'][j][k] == 1:\n",
    "                        act = k\n",
    "                for index in indices:\n",
    "                    c[act][index] += 1\n",
    "    print(c)\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1_score = {}\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    f1_score_sum = 0\n",
    "    for k in range(4):\n",
    "        precision[k] = c[k][k] / (c[0][k] + c[1][k] + c[2][k] + c[3][k]) if c[k][k] != 0 else 0\n",
    "        recall[k] = c[k][k] / (c[k][0] + c[k][1] + c[k][2] + c[k][3]) if c[k][k] != 0 else 0\n",
    "        f1_score[k] = 2 * precision[k] * recall[k] / (precision[k] + recall[k]) if (precision[k] + recall[k] != 0) else 0\n",
    "        precision_sum += precision[k]\n",
    "        recall_sum += recall[k]\n",
    "        f1_score_sum += f1_score[k]\n",
    "    if show:\n",
    "        for k in range(4):\n",
    "            print(f\"Precision@{LABEL_BACK_DICT[k]} = {precision[k]}, Recall@{LABEL_BACK_DICT[k]} = {recall[k]}, F1-score@{LABEL_BACK_DICT[k]} = {f1_score[k]}\")\n",
    "    return {'precision': precision_sum / 4, 'recall': recall_sum / 4, 'f1_score': f1_score_sum / 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, get_scheduler\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "try:\n",
    "    get_ipython().__class_._name__\n",
    "    from tqdm.notebook import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "RANDOM_SEED = 20\n",
    "LEARNING_RATE = 5e-5\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "BATCHSTEP_PER = 4\n",
    "DATASET_TRAIN = '../data/raw/medium_link_prediction_noClue_shuffled_train.csv'\n",
    "DATASET_TEST = '../data/raw/medium_link_prediction_noClue_shuffled_test.csv'\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def train_one_epoch(train_dataloader, model, optimizer, lr_scheduler, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        input_title1 = data['title1']\n",
    "        input_code1 = data['code1']\n",
    "        input_desc1 = data['desc1']\n",
    "        input_title2 = data['title2']\n",
    "        input_code2 = data['code2']\n",
    "        input_desc2 = data['desc2']\n",
    "        target = data['class'].cuda()\n",
    "        output = model(input_title1, input_code1, input_desc1, input_title2, input_code2, input_desc2)[0]\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        if (i + 1) % BATCHSTEP_PER == 0:\n",
    "            print(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "def train(model, optimizer, lr_scheduler, criterion, train_dataset, val_dataset):\n",
    "    setup_seed(RANDOM_SEED)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=0, shuffle=True, pin_memory=True)\n",
    "    for epoch, _ in enumerate(tqdm(range(EPOCHS), total=EPOCHS)):\n",
    "        train_one_epoch(train_dataloader, model, optimizer, lr_scheduler, criterion)\n",
    "        acc = val(model, val_dataset, show=True)\n",
    "        print(acc)\n",
    "        torch.save(model.state_dict(), datetime.now().strftime('%Y-%m-%d %H%M%S') + f'-epoch{epoch}.dat')\n",
    "    return acc\n",
    "\n",
    "# from models.BtdCModel import TagRecommandModel\n",
    "# model = RelatednessModelCross(TagRecommandModel, 23687, pretrained_model_path='大数据量/2022-04-08 041514-epoch1.dat', freeze_embedder=True, hidden_size=4096, code_model=True)\n",
    "\n",
    "# from models.BtdModel import TagRecommandModel\n",
    "# model = RelatednessModelCross(TagRecommandModel, 23687, pretrained_model_path='大数据量/2022-03-24 145103-epoch1.dat', freeze_embedder=True, hidden_size=4096)\n",
    "\n",
    "# model = RelatednessModel('roberta-base', freeze_embedder=False, cls_token_id=0, sep_token_id=2)\n",
    "\n",
    "# model = RelatednessModel('bert-base-uncased', freeze_embedder=True)\n",
    "\n",
    "# model = RelatednessModel('jeniya/BERTOverflow', freeze_embedder=False)\n",
    "\n",
    "# model = RelatednessModel('albert-base-v2', freeze_embedder=True)\n",
    "\n",
    "# model = RelatednessModel('./pre-training/roberta-tapt', freeze_embedder=False)\n",
    "# model = RelatednessModelCross('./pre-training/roberta-tapt', freeze_embedder=False, cls_token_id=0, sep_token_id=2)\n",
    "model = RelatednessModelCrossHidden('./pre-training/roberta-tapt', freeze_embedder=False, cls_token_id=0, sep_token_id=2, used_hidden_layer=12)\n",
    "\n",
    "# model = RelatednessModelCross('./pre-training/bertoverflow-tapt', freeze_embedder=True)\n",
    "\n",
    "model = model.cuda()\n",
    "print(model)\n",
    "\n",
    "train_dataset = Dataset(DATASET_TRAIN)\n",
    "val_dataset = Dataset(DATASET_TEST)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_training_steps = num_training_steps = EPOCHS * (len(train_dataset) // BATCHSTEP_PER)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "es = train(model, optimizer, lr_scheduler, criterion, train_dataset, val_dataset)\n",
    "print(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import load, no_grad, topk, arange\n",
    "from models.RelatednessModel import RelatednessModel\n",
    "from torch.utils.data import DataLoader\n",
    "DATASET_TEST = '../data/raw/medium_link_prediction_noClue_shuffled_test-mini.csv'\n",
    "\n",
    "model = RelatednessModelCross('./pre-training/roberta-tapt', freeze_embedder=False, cls_token_id=0, sep_token_id=2)\n",
    "model.load_state_dict(load('C:\\\\Users\\\\3090\\\\Documents\\\\相似比对\\\\cross-encoder\\\\Roberta-TAPT，不锁，epoch3\\\\2022-07-09 185026-epoch2.dat'))\n",
    "model = model.cuda()\n",
    "\n",
    "val_dataset = Dataset(DATASET_TEST)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, num_workers=0, shuffle=False)\n",
    "\n",
    "c = [[0 for j in range(4)] for i in range(4)]\n",
    "model.eval()\n",
    "act = []\n",
    "pred = []\n",
    "with no_grad():\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        print(data)\n",
    "        input_title1 = data['title1']\n",
    "        input_code1 = data['code1']\n",
    "        input_desc1 = data['desc1']\n",
    "        input_title2 = data['title2']\n",
    "        input_code2 = data['code2']\n",
    "        input_desc2 = data['desc2']\n",
    "        probability = model(input_title1, input_code1, input_desc1, input_title2, input_code2, input_desc2)[0]\n",
    "        tops = topk(probability, 1, sorted=True)\n",
    "        for indices, j in zip(tops.indices, arange(len(tops.indices))):\n",
    "            for k in range(4):\n",
    "                if data['class'][j][k] == 1:\n",
    "                    act.append(k)\n",
    "            for index in indices:\n",
    "                pred.append(index.item())\n",
    "\n",
    "with open('right.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(act)):\n",
    "        if act[i] == pred[i]:\n",
    "            f.write(f'{i} {pred[i]} {act[i]}\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72aa4cf4ad1c51c56d4fc6bd252654137ba59563fc873bbc471f3ad44b11c76e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
